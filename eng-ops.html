<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
    <title>The End of the Eng-Ops Split</title>
    <meta name="title" content="The End of the Eng-Ops Split">
    <meta name="description" content="The eng-ops split does not work for tasks at the frontier of LLM capability.">
    
    <!-- Open Graph / Social Media -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://benguzovsky.com/train-test.html">
    <meta property="og:title" content="The End of the Train-Test Split">
    <meta property="og:description" content="The eng-ops split does not work for tasks at the frontier of LLM capability.">
    <meta property="og:image" content="https://benguzovsky.com/images/train_test_og.webp">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    
    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://benguzovsky.com/train-test.html">
    <meta property="twitter:title" content="The End of the Train-Test Split">
    <meta property="twitter:description" content="The eng-ops split does not work for tasks at the frontier of LLM capability.">
    <meta property="twitter:image" content="https://benguzovsky.com/images/train_test_og.webp">
    
    <link rel="shortcut icon" href="/favicon/favicon.ico" type="image/x-icon">
    <link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
    <link rel="manifest" href="/favicon/site.webmanifest">
    
    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lora:wght@400;500;600;700&family=Inter:wght@400;500;600&display=swap" rel="stylesheet">
    
    <!-- Blog Styles -->
    <link rel="stylesheet" href="/blog-styles.css">
    
<!-- Privacy-friendly analytics by Plausible -->
<script async src="https://plausible.io/js/pa-DALALMGEq95sSTulrzwgY.js"></script>
<script>
  window.plausible=window.plausible||function(){(plausible.q=plausible.q||[]).push(arguments)},plausible.init=plausible.init||function(i){plausible.o=i||{}};
  plausible.init()
</script>
</head>
<body>
    <div class="blog-container">
        <!-- Blog Header -->
        <div class="blog-header">
            <a href="/">Folio</a>
            <span class="post-date">December 2025</span>
        </div>

        <!-- Title Section -->
        <div class="title-section">
            <h1>The End of the Eng-Ops Split</h1>
        </div>

        <!-- Content Section -->
        <div class="content-section">
            <h2>2015: Loosely based on a true story.</h2>

            <p>You are a machine learning engineer at Facebook in Menlo Park. Your task: build the best butt classification model, which decides if there is an exposed butt in an image.            </p>

            <p>The content policy team in D.C. has written country-specific censorship rules based on cultural tolerance for gluteal cleft—or butt crack, for the uninitiated.</p>

            <ul>
                <li>Germany: 0% cleft.</li>
                <li>Zimbabwe: 30% cleft.</li>
                <li>Cupertino: 0%.</li>
                <li>Montana: 20%.</li>
            </ul>

            <p>A PM on your team writes data labeling guidelines for a business process outsourcing firm (BPO), and each example in your dataset is triple-reviewed by the firm's outsourced team to ensure consistency. You skim the labels, which seem reasonable.</p>

            <p>You decide to train a CNN: it'll be perfect for this edge detection task. Two months later, you've cracked it. Your model goes live, great success, 92% precision, 98% recall. You never once had to talk to the policy team in D.C.</p>

            <h2>2023: The butt model has been in production for 8 years.</h2>

            <p>Another email: Policy has heard about LLMs and thinks it's time to build a more "context-aware" model. They would like the model to understand whether there is sexually suggestive posing, sexual context, or artistic intent in the image.</p>

            <p>You receive a 10 page policy doc. The PM cleans it up a bit and sends it to the BPO. The data is triple reviewed, you skim the labels, and they seem fine.</p>

            <p>You make an LLM decision tree, one LLM call per policy section, and aggregate the results. Two months pass. You are stuck at 85% precision and recall, no matter how much prompt engineering and model tuning you do. You try going back to a CNN, training it on the labels. It scores 83%.</p>

            <p>Your data science spidey-sense tingles. Something is wrong with the labels or the policy.</p>

            <p>You email the policy team, sending them the dataset and results.</p>

            <p>The East Coast Metamates say they looked at 20 of the labels. 60% were good, 20% were wrong, 20% were edge cases they hadn’t thought of.</p>

            <p>The butt model lives to fight another day, or at least until these discrepancies get sorted out.</p>

            <h2>2025.</h2>

            <p>The butt model is still in production… What went wrong?</p>

            <h3><strong>The eng-ops split does not work for tasks at the frontier of LLM capability.</strong></h3>

            <p>Physical and organizational distance between engineering and policy may have worked for simple machine learning, but creates insurmountable challenges for tasks at the frontier of LLM capability, where engineering and policy decision-making are highly coupled. It doesn't matter if the task is content policy, sales chatbots, legal chatbots, or AI automation in any other industry.</p>

            <p><strong>Operations, Product, and Policy must be involved in LLM integration:</strong> Any task complicated enough to require an LLM will need policy expert labels.<sup id="ref-1"><a href="#footnote-1">[1]</a></sup> To detect nudity, outsourced labels will do, and an engineer can eyeball results without much trouble. To enforce a 10 page adult policy, a policy expert must label the data, look at results, and collaborate with engineers to determine the best AI integration.</p>

            <p>Is a discrepancy between the LLM result and the ground truth due to the LLM's mistake, the labeler's mistake, or an ambiguous policy line? Resolving these questions — not only explaining policy rules, but arbitrating new edge cases — is the primary blocker for LLM integration, not tuning hyperparameters or trying novel RL algorithms.<sup id="ref-2"><a href="#footnote-2">[2]</a></sup> Policy experts have little time, but LLM integration requires these unprecedented levels of communication.</p>

            <h3>Core problems with complex classification tasks</h3>

            <p><strong>Policy Experts write abstract rules.</strong> For older classification tasks, the rules <em>had</em> to be simple, because simple was all that models could do. Are there guns in the image? Is there a minor in the image? Complicated tasks are harder to pin down.</p>

            <ul>
                <li>What is hate speech?</li>
                <li>What is sexually suggestive?</li>
                <li>What makes false advertising cross from overdone to illegal?</li>
            </ul>
            <p>Take these examples. Sexually suggestive? Artistic expression? Sufficient censorship?</p>
            <img src="/images/solar_power.webp" alt="Policy example" class="blog-image">
            <p class="caption">Source: Lorde's album covers from Vice.com, where you can find discussion of what is visible or not in each photo.</p>

            <p><strong>Most policy documents are not well-kept.</strong> A content policy is typically simplified, operationalized, and sent to outsourced teams to enforce. Edge cases and discrepancies found in India never make it back to policy teams in DC, so abstract rules are rarely pinned down.</p>

            <p>In production, we see 15-20% false positive rates on BPOs. Half are attributable to human error, half to policy gray area.</p>

            <p><strong>To resolve edge cases, labeling tasks require an expert's time.</strong> BPO agents can eyeball how much butt is visible, but struggle with what is "sexually suggestive." BPOs are low-wage workers in countries like India or the Phillipines, and may have different definitions of "sexual context" than the policy writers intended. The costs of training them are often prohibitive at scale.</p>

            <p>Using in-house agents is not sufficient for training data, either, as small alignment issues in the dataset cause large issues in production: If internal agents are 95% accurate (pretty good), the ceiling for the LLM's performance is 95%. If the LLM gets 95% of <em>those</em> labels right, its accuracy will be 90%.</p>

            <p><strong>Hard classification tasks have high rates of expert disagreement.</strong> Ask two people if there's a gun in an image, odds are they'll agree. Ask two policy experts whether a pose is sexually suggestive per their definition, you will start a one hour debate. If two experts only agree 95% of the time, then hand off to internal agents for labeling at 95% accuracy, then the LLM is 95% accurate, you are down to 86% LLM accuracy.</p>

            <p><strong>Language models see details in data that even experts miss.</strong> LLMs read every word of the product description. They scrutinize every image for a single pixel of gluteal cleft. They see gluteal cleft through sheer clothing, in low opacity, and in the bottom left corner of the back of a t-shirt. Even if experts have reviewed the data, there must be a re-review and feedback loop to check what the LLM has flagged.</p>

            <p><strong>Since labels are often wrong or ambiguous, you must review discrepancies between the LLM and the ground truth.</strong> Did the expert missed something, was the policy ambiguous, or was the outsourced labeler wrong? You have to check the results and the LLM's explanation to find out.</p>

            <p>In production, we see anywhere from 15-30% error rates in data considered "golden sets" by operations teams.</p>

            <p><strong>Since these classification tasks are so complex, you can only "debug" the model by explicating the policy rules in more detail for the LLM's understanding.</strong> A policy might have dozens of rules and thousands of possible inputs, creating a fat-tail of model mistakes. Unlike traditional machine learning, where you fix mistakes by changing the design or hyperparameters of your model, you fix LLM mistakes by changing the prompt. You can <em>directly</em> fix a mistake (e.g. by telling the model "do not consider spreading legs fully clothed to be sexually suggestive"), so fixing mistakes is fundamentally a policy task, not an engineering one.<sup id="ref-4"><a href="#footnote-4">[4]</a></sup></p>

            <p><strong>The Policy and Engineering Teams Need to be in Direct, Frequent Communication.</strong> The SF-DC split doesn't work anymore. Resolving edge cases and, in many cases, changing the policy to reflect patterns identified in the data requires collaboration.</p>

            <p><strong>Experts have historically not needed to look at the data—seen as a low-status task—but it is the only way to achieve high accuracy.</strong> This is an unsolved problem in many large organizations that blocks LLM integrations.</p>

            <p><strong>The "training" step for language models has to be policy alignment, not heating up GPUs.</strong> Since the data will always be flawed and the test set won't be blind, the machine learning engineer's priority should be spent working with policy teams to improve the data. That means surfacing edge cases and policy gray areas, clarifying policy definitions, and leveraging LLM outputs to find more discrepancies until data is high-quality and policy is clear.</p>

            <p>In production, this is an ongoing process, as LLMs will always surface new interesting cases and policies will continue to change. Policies and enforcement are better for this feedback loop: it enables consistent, scaled enforcement platform-wide.</p>

            <h2>Today and Tomorrow</h2>

            <p>This is a paradigm shift that many machine learning teams, and enterprises as a whole, have not yet embraced. It requires a complete change in how we approach classification tasks.</p>

            <p><strong>If this is the road to automation, is it even worthwhile?</strong> The process described above, while arduous, is the shortest route to consistent policy enforcement to date. Before LLMs, running a successful quality assurance program would be prohibitively expensive. Retraining human agents takes far longer than retraining LLMs. Policy experts have historically never been owners in quality assurance processes, but now can be.</p>

            <p>To save a little time, an in-house human agent might do a first review of the results, then a policy expert can review only the discrepancies. We find this tradeoff works well in production.</p>

            <p><strong>What are the implications for leveraging LLMs for tasks which <em>do not have binary classifications</em>?</strong> Can an LLM be a lawyer if this much work is required to align, evaluate, and test models? Will an LLM ever ~know what you mean~ and skip all these alignment steps?</p>

            <p>One core problem with the LLM architecture is that the model doesn't know when it is wrong. Model improvements over the past few years mean the LLM is right more often, but when it is wrong, it doesn't have an outlet.</p>

            <p>This is a perennial machine learning problem: a model does not know what is "out of distribution" for itself.</p>

            <p>Until that problem is solved, there will have to be an engineer in the loop improving and testing the model, and a policy expert evaluating the results. You can do this for complicated tasks like writing a patent application, but you have to be rigorous, define a rubric, curate expert data, and regularly evaluate model outputs. Calculating accuracy of each "training run" will never be as easy as checking if model_output == ground_truth, and will require a human in the loop. These complex tasks are far more lucrative than binary classification, and smart people are working on them.</p>

            <p>Not everybody will take this rigorous approach, and as models improve, they might not have to. Until then, the highest leverage way to spend your time in 2026 will be looking closely at your data, cleaning your data, and labeling your data.</p>

            <!-- Footnotes -->
            <div class="footnotes">
                <div class="footnote-item" id="footnote-1">
                    <sup><a href="#ref-1">[1]</a></sup> A policy expert may literally be someone with “Policy” in their title, or it may be a lawyer, accountant, sales rep, or anyone else documenting a business process that needs to be automated.
                </div>
                <div class="footnote-item" id="footnote-2">
                    <sup><a href="#ref-2">[2]</a></sup> Note that LLMs can be trained or tuned on specific datasets for classification tasks, especially reasoning models, but these tend to be much smaller, higher quality labels rather than scaled, medium quality labels. <i>Even in these cases, an engineer must spend a large amount of time looking at “test set” data and labels.</i>
                </div>
                <div class="footnote-item" id="footnote-3">
                    <sup><a href="#ref-3">[3]</a></sup> After several runs of this prompt-based iteration following by QA on unseen, production data, you may have enough data for real LLM training on a train-test split.
                </div>
            </div>
        </div>

        <!-- Large Footer -->
        <div class="footer"></div>
    </div>
</body>
</html>

